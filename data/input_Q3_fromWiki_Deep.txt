Abstract
Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction These methods have dramatically improved the state-of-the-art in speech recognition visual object recognition object detection and many other domains such as drug discovery and genomics Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer Deep convolutional nets have brought about breakthroughs in processing images video speech and audio whereas recurrent nets have shone light on sequential data such as text and speech
Main
Machine-learning technology powers many aspects of modern society  from web searches to content filtering on social networks to recommendations on e-commerce websites and it is increasingly present in consumer products such as cameras and smartphones Machine-learning systems are used to identify objects in images transcribe speech into text match news items posts or products with users interests and select relevant results of search Increasingly these applications make use of class of techniques called deep learning
Conventional machine-learning techniques were limited in their ability to process natural data in their raw form For decades constructing pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design feature extractor that transformed the raw data such as the pixel values of an image into suitable internal representation or feature vector from which the learning subsystem often classifier could detect or classify patterns in the input
Representation learning is set of methods that allows machine to be fed with raw data and to automatically discover the representations needed for detection or classification Deep-learning methods are representation-learning methods with multiple levels of representation obtained by composing simple but non-linear modules that each transform the representation at one level starting with the raw input into representation at higher slightly more abstract level With the composition of enough such transformations very complex functions can be learned For classification tasks higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations An image for example comes in the form of an array of pixel values and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image The second layer typically detects motifs by spotting particular arrangements of edges regardless of small variations in the edge positions The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects and subsequent layers would detect objects as combinations of these parts The key aspect of deep learning is that these layers of features are not designed by human engineers  they are learned from data using general-purpose learning procedure
Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science business and government In addition to beating records in image recognition1234 and speech recognition567 it has beaten other machine-learning techniques at predicting the activity of potential drug molecules8 analysing particle accelerator data reconstructing brain circuits11 and predicting the effects of mutations in non-coding DNA on gene expression and disease1213 Perhaps more surprisingly deep learning has produced extremely promising results for various tasks in natural language understanding14 particularly topic classification sentiment analysis question answering15 and language translation1617
We think that deep learning will have many more successes in the near future because it requires very little engineering by hand so it can easily take advantage of increases in the amount of available computation and data New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress
Supervised learning
The most common form of machine learning deep or not is supervised learning Imagine that we want to build system that can classify images as containing say house car person or pet We first collect large data set of images of houses cars people and pets each labelled with its category During training the machine is shown an image and produces an output in the form of vector of scores one for each category We want the desired category to have the highest score of all categories but this is unlikely to happen before training We compute an objective function that measures the error or distance between the output scores and the desired pattern of scores The machine then modifies its internal adjustable parameters to reduce this error These adjustable parameters often called weights are real numbers that can be seen as  that define the input–output function of the machine In typical deep-learning system there may be hundreds of millions of these adjustable weights and hundreds of millions of labelled examples with which to train the machine
To properly adjust the weight vector the learning algorithm computes gradient vector that for each weight indicates by what amount the error would increase or decrease if the weight were increased by tiny amount The weight vector is then adjusted in the opposite direction to the gradient vector
The objective function averaged over all the training examples can be seen as kind of hilly landscape in the high-dimensional space of weight values The negative gradient vector indicates the direction of steepest descent in this landscape taking it closer to minimum where the output error is low on average
In practice most practitioners use procedure called stochastic gradient descent SGD This consists of showing the input vector for few examples computing the outputs and the errors computing the average gradient for those examples and adjusting the weights accordingly The process is repeated for many small sets of examples from the training set until the average of the objective function stops decreasing It is called stochastic because each small set of examples gives noisy estimate of the average gradient over all examples This simple procedure usually finds good set of weights surprisingly quickly when compared with far more elaborate optimization techniques18 After training the performance of the system is measured on different set of examples called test set This serves to test the generalization ability of the machine — its ability to produce sensible answers on new inputs that it has never seen during training
Many of the current practical applications of machine learning use linear classifiers on top of hand-engineered features two-class linear classifier computes weighted sum of the feature vector components If the weighted sum is above threshold the input is classified as belonging to particular category
Since the we have known that linear classifiers can only carve their input space into very simple regions namely half-spaces separated by hyperplane But problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input such as variations in position orientation or illumination of an object or variations in the pitch or accent of speech while being very sensitive to particular minute variations for example the difference between white wolf and breed of wolf-like white dog called Samoyed At the pixel level images of two Samoyeds in different poses and in different environments may be very different from each other whereas two images of Samoyed and wolf in the same position and on similar backgrounds may be very similar to each other linear classifier or any other  classifier operating on raw pixels could not possibly distinguish the latter two while putting the former two in the same category This is why shallow classifiers require good feature extractor that solves the selectivity–invariance dilemma — one that produces representations that are selective to the aspects of the image that are important for discrimination but that are invariant to irrelevant aspects such as the pose of the animal To make classifiers more powerful one can use generic non-linear features as with kernel methods but generic features such as those arising with the Gaussian kernel do not allow the learner to generalize well far from the training examples21 The conventional option is to hand design good feature extractors which requires considerable amount of engineering skill and domain expertise But this can all be avoided if good features can be learned automatically using general-purpose learning procedure This is the key advantage of deep learning
A deep-learning architecture is multilayer stack of simple modules all or most of which are subject to learning and many of which compute non-linear input–output mappings Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation With multiple non-linear layers say depth of to system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations such as the background pose lighting and surrounding objects
Backpropagation to train multilayer architectures
From the earliest days of pattern recognition the aim of researchers has been to replace hand-engineered features with trainable multilayer networks but despite its simplicity the solution was not widely understood until the mid As it turns out multilayer architectures can be trained by simple stochastic gradient descent As long as the modules are relatively smooth functions of their inputs and of their internal weights one can compute gradients using the backpropagation procedure The idea that this could be done and that it worked was discovered independently by several different groups during the and
The backpropagation procedure to compute the gradient of an objective function with respect to the weights of multilayer stack of modules is nothing more than practical application of the chain rule for derivatives The key insight is that the derivative or gradient of the objective with respect to the input of module can be computed by working backwards from the gradient with respect to the output of that module or the input of the subsequent module Fig The backpropagation equation can be applied repeatedly to propagate gradients through all modules starting from the output at the top where the network produces its prediction all the way to the bottom where the external input is fed Once these gradients have been computed it is straightforward to compute the gradients with respect to the weights of each module